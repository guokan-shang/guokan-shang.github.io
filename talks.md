---
layout: page
title: Talks
subtitle: Here is a list of my talks and presentations.
---

**2023**

- **Large Language Models: from "Attention Is All You Need" to "ChatGPT"**   
  *work-in-progress*, I'm giving a serie of talks about LLMs, the slides will be continuously updated.  
  [Overleaf](https://www.overleaf.com/read/rvwwvvwmxvyc){: .btn .btn-outline-primary .btn-sm}{:target="_blank"}
  - Transformer, BERT, BART, GPT 
  - InstructGPT / ChatGPT = GPT + Instruction-tuning + Alignment-tuning / Reinforcement Learning from Human Feedback (RLHF) 
  - Parameter-Efficient Fine-tuning (PEFT): Adapter-tuning, Prefix-tuning, Low-Rank Adaptation (LoRA)  
  - Quantization: 8-bit optimizer, LLM.int8(), GPTQ
  - Attention with Linear Biases (ALiBi)  
 
